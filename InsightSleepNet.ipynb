{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f55a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: borum\n",
    "\n",
    "TCN code from: https://github.com/locuslab/TCN\n",
    "Energy OOD code from: https://github.com/wetliu/energy_ood\n",
    "ResNet code from: https://github.com/hsd1503/resnet1d\n",
    "InceptionTime code from: https://github.com/TheMrGhostman/InceptionTime-Pytorch\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from local_attention import LocalAttention\n",
    "from net1d import Net1D\n",
    "from inception import Inception, InceptionBlock\n",
    "from torchsummary import summary\n",
    "\n",
    "\"\"\"\n",
    "@author: borum\n",
    "\n",
    "TCN code from: https://github.com/locuslab/TCN\n",
    "Energy OOD code from: https://github.com/wetliu/energy_ood\n",
    "ResNet code from: https://github.com/hsd1503/resnet1d\n",
    "InceptionTime code from: https://github.com/TheMrGhostman/InceptionTime-Pytorch\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from local_attention import LocalAttention\n",
    "from net1d import Net1D\n",
    "from inception import Inception, InceptionBlock\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "GPU_NUM = 0\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "\n",
    "MESA_PPG_PATH = '/PPG_SleepStaging/MESA_PPG_preprocessed'\n",
    "MESA_annot_PATH = '/PPG_SleepStaging/MESA_annot_preprocessed'\n",
    "\n",
    "\n",
    "to_np = lambda x: x.data.cpu().numpy()\n",
    "\n",
    "#Training parameters\n",
    "BATCH_SIZE = 2\n",
    "LR = 1e-3\n",
    "max_epoch = 100\n",
    "k_fold=3\n",
    "fold_n = 1 ## hold-out --> fold_n = 1 \n",
    "\n",
    "# Labeling (str to int)\n",
    "def sleep_label(annot_array):\n",
    "    total_len = np.shape(annot_array)[0]\n",
    "    for index in range(total_len):\n",
    "        if annot_array[index] == 0:\n",
    "            annot_array[index] = 0\n",
    "        elif annot_array[index] == 1:\n",
    "            annot_array[index] = 1\n",
    "        elif annot_array[index] == 2:\n",
    "            annot_array[index] = 1\n",
    "        elif annot_array[index] == 3:\n",
    "            annot_array[index] = 2\n",
    "        elif annot_array[index] == 4:\n",
    "            annot_array[index] = 2\n",
    "        elif annot_array[index] == 5:\n",
    "            annot_array[index] = 3\n",
    "        else:\n",
    "            pass\n",
    "    return annot_array\n",
    "\n",
    "#k-fold\n",
    "def k_fold(k,fold_num,file_path):\n",
    "    subject_list = []\n",
    "    for subject in sorted(os.listdir(file_path)):\n",
    "        subject_list.append(subject)\n",
    "    train_subject_set = []\n",
    "    test_subject_set = []\n",
    "    samples_in_fold = int(round(len(subject_list)/k))\n",
    "    if not len(subject_list)%samples_in_fold == 0:\n",
    "        supplement_num = samples_in_fold - (len(subject_list)%samples_in_fold)\n",
    "        for sup in list(range(supplement_num)):\n",
    "            subject_list.append(subject_list[sup])\n",
    "        test_subject_set = subject_list[samples_in_fold*(fold_num-1):samples_in_fold*fold_num]\n",
    "        train_subject_set = [x for x in subject_list if x not in test_subject_set]\n",
    "    else:\n",
    "        test_subject_set = subject_list[samples_in_fold*(fold_num-1):samples_in_fold*fold_num]\n",
    "        train_subject_set = [x for x in subject_list if x not in test_subject_set]\n",
    "    return train_subject_set, test_subject_set\n",
    "\n",
    "def truncateORpad(arr, annot=False):\n",
    "    if annot==False:\n",
    "        if np.shape(arr)[0] < 1228800:\n",
    "            arr = np.pad(arr, ((0, 1228800-np.shape(arr)[0]),(0,0)), 'constant', constant_values=0)\n",
    "        elif np.shape(arr)[0] > 1228800:\n",
    "            arr = arr[:1228800]\n",
    "            \n",
    "        else:\n",
    "            pass       \n",
    "    elif annot is True:\n",
    "        if np.shape(arr)[0] < 1200:\n",
    "            arr = np.pad(arr, ((0,1200-np.shape(arr)[0]),(0,0)), 'constant', constant_values=4)\n",
    "        elif np.shape(arr)[0] >1200:\n",
    "            arr = arr[:1200]\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    return arr\n",
    "\n",
    "    \n",
    "# DB load function\n",
    "def data_to_dict(ppg_data_path,annot_path, fold_num):\n",
    "    ppg_train_sample_list = []\n",
    "    ppg_test_sample_list = []\n",
    "    train_subject_list = []\n",
    "    test_subject_list = []\n",
    "    train_class_dict = {}\n",
    "    test_class_dict = {}\n",
    "    \n",
    "    train_subject_list, test_subject_list = k_fold(k=3,fold_num=fold_num,file_path=ppg_data_path)\n",
    "    \n",
    "    # train set\n",
    "    for train_subject in train_subject_list:\n",
    "        train_name = train_subject.split('.')[0]\n",
    "        train_annot_df = pd.read_csv(annot_path+'/'+train_name+'-profusion.csv',header=None)\n",
    "        train_annot_array = np.array(train_annot_df)\n",
    "        train_label = sleep_label(train_annot_array)\n",
    "        ppg_train_sample_path = ppg_data_path+'/'+train_subject\n",
    "        ppg_train_sample_list.append(ppg_train_sample_path)\n",
    "        train_class_dict[ppg_train_sample_path] = train_label\n",
    "    # test set\n",
    "    for test_subject in test_subject_list:\n",
    "        test_name = test_subject.split('.')[0]\n",
    "        test_annot_df = pd.read_csv(annot_path+'/'+test_name+'-profusion.csv',header=None)\n",
    "        test_annot_array = np.array(test_annot_df)\n",
    "        test_label = sleep_label(test_annot_array)\n",
    "        ppg_test_sample_path = ppg_data_path+'/'+test_subject\n",
    "        ppg_test_sample_list.append(ppg_test_sample_path)\n",
    "        test_class_dict[ppg_test_sample_path] = test_label\n",
    "    return train_class_dict, ppg_train_sample_list, test_class_dict, ppg_test_sample_list\n",
    "\n",
    "\n",
    "class PSG_DB():\n",
    "    def __init__(self, category_dict, ppg_sample_dirs):\n",
    "        self.category_dict = category_dict\n",
    "        self.ppg_sample_dirs = ppg_sample_dirs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ppg_sample_dirs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ppg_sample = self.ppg_sample_dirs[idx]\n",
    "        ppg_sample_df = pd.read_csv(ppg_sample,header=None)\n",
    "        ppg_sample_input = np.array(ppg_sample_df)\n",
    "        ppg_sample_input = truncateORpad(ppg_sample_input)\n",
    "        label_per_input = self.category_dict[ppg_sample]\n",
    "        label_per_input = truncateORpad(label_per_input, annot=True)\n",
    "        return ppg_sample_input, label_per_input\n",
    "    \n",
    "# Load Data\n",
    "train_class, ppg_train_list, test_class, ppg_test_list = data_to_dict(\n",
    "    MESA_PPG_PATH, MESA_annot_PATH, fold_num=fold_n)\n",
    "train_dataset = PSG_DB(train_class, ppg_train_list)\n",
    "test_dataset = PSG_DB(test_class, ppg_test_list)\n",
    "\n",
    "train_total_data_num = len(ppg_train_list)\n",
    "test_total_data_num = len(ppg_test_list)           \n",
    "\n",
    "train_data_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "test_data_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "        y = self.module(x_reshape)\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "        return y\n",
    "    \n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    def forward(self,x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "    \n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.1):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                          stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                          stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "    \n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=20, dropout=0.1):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(n_inputs=in_channels, n_outputs=out_channels, \n",
    "                                     kernel_size=kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x) \n",
    "    \n",
    "class SleepStaging_NET(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(SleepStaging_NET, self).__init__()\n",
    "        # Local attention mask\n",
    "        self.causal_conv =  weight_norm(nn.Conv1d(1, 8, kernel_size=7168,\n",
    "                                                  stride=1, padding=7167, dilation=1))\n",
    "        self.chomp_att = Chomp1d(7167)\n",
    "        self.downsample_att = nn.Conv1d(8, 1, 1)\n",
    "        self.local_att = nn.Sequential(self.causal_conv, self.chomp_att, self.downsample_att)\n",
    "        \n",
    "        # InceptionTime\n",
    "        self.conv1d = nn.Conv1d(1, 32, kernel_size=40, stride=20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.InceptionTime = nn.Sequential(\n",
    "            InceptionBlock(\n",
    "                in_channels=32, \n",
    "                n_filters=8, \n",
    "                kernel_sizes=[5,11,23],\n",
    "                bottleneck_channels=8,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            InceptionBlock(\n",
    "                in_channels=32, \n",
    "                n_filters=16, \n",
    "                kernel_sizes=[5,11,23],\n",
    "                bottleneck_channels=16,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            InceptionBlock(\n",
    "                in_channels=64, \n",
    "                n_filters=16, \n",
    "                kernel_sizes=[5,11,23],\n",
    "                bottleneck_channels=16,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            InceptionBlock(\n",
    "                in_channels=64, \n",
    "                n_filters=32, \n",
    "                kernel_sizes=[5,11,23],\n",
    "                bottleneck_channels=16,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            InceptionBlock(\n",
    "                in_channels=128, \n",
    "                n_filters=64, \n",
    "                kernel_sizes=[5,11,23],\n",
    "                bottleneck_channels=32,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            InceptionBlock(\n",
    "                in_channels=256, \n",
    "                n_filters=128, \n",
    "                kernel_sizes=[5,11,23],\n",
    "                bottleneck_channels=32,\n",
    "                use_residual=True,\n",
    "                activation=nn.ReLU()\n",
    "            ),\n",
    "            nn.AdaptiveAvgPool1d(output_size=1200)\n",
    "        )\n",
    "        self.conv1 = nn.Conv1d(512, 256, kernel_size=1)\n",
    "\n",
    "        # Time-distributed dense layer\n",
    "        self.tdd =TimeDistributed(nn.Linear(256,128),batch_first=True)\n",
    "        # TCN\n",
    "        self.tcn = TemporalConvNet(128, [64,64,64,64,64], kernel_size=8, dropout=0.2)\n",
    "        self.downsample = nn.Conv1d(64, 4, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # input shape: (batch_size, time_step, feature)\n",
    "        inputs = inputs.transpose(2, 1) # now input shape : (batch_size, feature, time_step)\n",
    "        att = self.local_att(inputs) # att shape : (batch_size, feature, time_step)\n",
    "        for i in range(1200):\n",
    "            att[:,:,i*1024:(i+1)*1024] = F.sigmoid(att[:,:,i*1024:(i+1)*1024].clone())\n",
    "        inputs = inputs*att\n",
    "        inputs = self.conv1d(inputs)\n",
    "        inputs = self.relu(inputs)\n",
    "        inception_output = self.InceptionTime(inputs)\n",
    "        inception_output = self.conv1(inception_output)\n",
    "        inception_output = inception_output.transpose(2, 1) # now inception_output shape : (batch_size, time_step, feature)\n",
    "        tdd_output = self.tdd(inception_output) # tdd_output shape : (batch_size, time_step, feature)\n",
    "        tdd_output = tdd_output.transpose(2, 1) # now tdd_output shape : (batch_size, feature, time_step)\n",
    "        tcn_output = self.tcn(tdd_output) # tcn_output shape : (batch_size, feature, time_step)\n",
    "        output = self.downsample(tcn_output) # output shape : (batch_size, feature, time_step)\n",
    "        \n",
    "        return F.log_softmax(output, dim=1), att, output\n",
    "\n",
    "def get_energy(output_array):\n",
    "    T = 1\n",
    "    energy = -to_np((T*torch.logsumexp(output_array / T, dim=1)))\n",
    "            \n",
    "    return energy\n",
    "                                                                                                                     \n",
    "def fit(batch_size,\n",
    "        data,\n",
    "        label,\n",
    "        model,\n",
    "        model_optimizer,\n",
    "       inference=False):\n",
    "    loss = 0\n",
    "    model_optimizer.zero_grad()\n",
    "\n",
    "    prediction_results, attention, final_feature = model.forward(data)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    label_len = []\n",
    "    mini_batch_size = np.shape(label)[0]\n",
    "    for batch in range(mini_batch_size):\n",
    "        for i in range(1200):\n",
    "            if label[batch,i] == 4:\n",
    "                label_len.append(i)\n",
    "                break\n",
    "            elif i == 1199:\n",
    "                label_len.append(1200)\n",
    "                break\n",
    "    \n",
    "    corrects = 0\n",
    "\n",
    "    for batch in range(mini_batch_size):\n",
    "        batch_prediction_results = prediction_results[batch,:,:label_len[batch]].clone()\n",
    "        batch_prediction_results = batch_prediction_results.transpose(1, 0) # shape: (timesteps, 4), 4 is class\n",
    "        batch_label = label[batch,:label_len[batch]].view([-1]).clone() # shape: (timesteps)\n",
    "        current_loss = loss_function(batch_prediction_results, batch_label)\n",
    "        loss += current_loss\n",
    "        corrects_sum = ((torch.argmax(batch_prediction_results, 1)) == torch.squeeze(batch_label)).sum()\n",
    "        corrects += corrects_sum\n",
    "    L = sum(label_len)\n",
    "        \n",
    "    attention = torch.squeeze(attention)\n",
    "    #loss.requires_grad_(True)\n",
    "    if inference is False:\n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "\n",
    "    final_loss = loss.item()\n",
    "    pred_label = torch.argmax(prediction_results, dim=1)\n",
    "    pred_label = torch.squeeze(pred_label)\n",
    "    energy_results = get_energy(final_feature)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return label, pred_label, prediction_results, energy_results, attention, \\\n",
    "            final_loss, corrects, L, final_feature\n",
    "\n",
    "\n",
    "log_name = 'InsightSleepNet(InceptionTime)MESA_batchsize_{0}_initlr_{1}_fold{2}'.format(str(BATCH_SIZE), str(LR), str(fold_n))+time.strftime(\"_%b_%d_%H_%M\", \n",
    "                                                                                          time.localtime())\n",
    "\n",
    "saved_weights_folder = os.path.join('/PPG_SleepStaging/results/MESA',log_name)\n",
    "if not os.path.exists(saved_weights_folder):\n",
    "    os.makedirs(saved_weights_folder)\n",
    "    \n",
    "net = SleepStaging_NET(input_size=1, output_size=4).to(device)\n",
    "\n",
    "optimizer = torch.optim.RMSprop(net.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer, \n",
    "                                                  lr_lambda= lambda epoch: 1.0 ** epoch)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "num_step_per_epoch_train = train_total_data_num/BATCH_SIZE\n",
    "num_step_per_epoch_test = test_total_data_num/BATCH_SIZE\n",
    "\n",
    "for epoch_num in range(max_epoch):\n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    train_corrects = 0\n",
    "    train_L = 0\n",
    "    train_label_pred_energy = np.zeros((1,1200,3))\n",
    "    train_att = np.zeros((1,1228800))\n",
    "    train_softmax = np.zeros((1,4,1200))\n",
    "    train_feature = np.zeros((1,4,1200))\n",
    "    for i, (ppg_input, label) in enumerate(train_data_loader):\n",
    "        ppg_input = Variable(ppg_input).cuda().float()\n",
    "        label = Variable(label).cuda().long()\n",
    "        \n",
    "        label_result, pred_label, pred, energy, att, loss, batch_corrects, batch_L, feature = fit(\n",
    "            BATCH_SIZE, ppg_input, label,net,optimizer)   \n",
    "        # attention stack by batch(subject)\n",
    "        train_att_tmp = att.cpu().data.numpy()\n",
    "        train_att = np.concatenate((train_att, train_att_tmp), axis=0)\n",
    "        # softmax stack by batch(subject)\n",
    "        train_softmax_tmp = pred.cpu().data.numpy()\n",
    "        train_softmax = np.concatenate((train_softmax, train_softmax_tmp), axis=0)\n",
    "        # final feature stack by batch(subject)\n",
    "        train_feature_tmp = feature.cpu().data.numpy()\n",
    "        train_feature = np.concatenate((train_feature,train_feature_tmp), axis=0)\n",
    "\n",
    "        # label, pred label, energy stack by batch(subject)\n",
    "        train_label_tmp = label_result.cpu().data.numpy()\n",
    "        train_pred_label_tmp = pred_label.cpu().data.numpy()\n",
    "        train_energy_tmp = energy\n",
    "        label_pred_energy_tmp = np.concatenate((train_label_tmp,\n",
    "                                               np.expand_dims(train_pred_label_tmp,2),\n",
    "                                               np.expand_dims(train_energy_tmp, 2)), axis=2)\n",
    "        train_label_pred_energy = np.concatenate((train_label_pred_energy,label_pred_energy_tmp), axis=0)\n",
    "\n",
    "        # loss, acc\n",
    "        epoch_train_loss += loss\n",
    "        train_corrects += batch_corrects\n",
    "        train_L += batch_L\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Train results save\n",
    "    saved_by_epoch = os.path.join(saved_weights_folder,'epoch_{}'.format(epoch_num, '04'))\n",
    "    if not os.path.exists(saved_by_epoch):\n",
    "        os.makedirs(saved_by_epoch)\n",
    "        \n",
    "    # attention, softmax, final output feature save\n",
    "    train_att_np = train_att[1:,:]\n",
    "    train_softmax_np = train_softmax[1:,:,:]\n",
    "    train_feature_np = train_feature[1:,:,:]\n",
    "    np.save(saved_by_epoch+\"/train_att.npy\", train_att_np)\n",
    "    np.save(saved_by_epoch+\"/train_softmax.npy\", train_softmax_np)\n",
    "    np.save(saved_by_epoch+\"/train_feature.npy\", train_feature_np)\n",
    "    \n",
    "    # label, pred label, energy\n",
    "    train_label_pred_energy = train_label_pred_energy[1:,:,:]\n",
    "    np.save(saved_by_epoch+\"/train_pred_results.npy\", train_label_pred_energy)\n",
    "    \n",
    "    # loss, acc\n",
    "    epoch_train_loss = epoch_train_loss/num_step_per_epoch_train\n",
    "    avg_train_acc = train_corrects.float()/float(train_L)\n",
    "    print(\"epoch:{} \".format(epoch_num)+ \" train loss: {}\".format(epoch_train_loss))\n",
    "    print(\"epoch:{} \".format(epoch_num)+ \" train accuracy: {}\".format(avg_train_acc))\n",
    "    train_loss_list.append(epoch_train_loss)\n",
    "    train_acc_list.append(avg_train_acc)\n",
    "    \n",
    "    epoch_test_loss = 0\n",
    "    test_corrects = 0\n",
    "    test_L = 0\n",
    "    test_label_pred_energy = np.zeros((1,1200,3))\n",
    "    test_att = np.zeros((1,1228800))\n",
    "    test_softmax = np.zeros((1,4,1200))\n",
    "    test_feature = np.zeros((1,4,1200))\n",
    "    for i, (ppg_input, label) in enumerate(test_data_loader):\n",
    "        with torch.no_grad():\n",
    "            ppg_input = Variable(ppg_input).cuda().float()\n",
    "            label = Variable(label).cuda().long()\n",
    "\n",
    "            label_result, pred_label, pred, energy, att, loss, batch_corrects, batch_L, feature = fit(\n",
    "                BATCH_SIZE, ppg_input, label,net,optimizer,inference=True)\n",
    "\n",
    "        # attention stack by batch(subject)\n",
    "        test_att_tmp = att.cpu().data.numpy()\n",
    "        test_att = np.concatenate((test_att, test_att_tmp), axis=0)\n",
    "        # softmax stack by batch(subject)\n",
    "        test_softmax_tmp = pred.cpu().data.numpy()\n",
    "        test_softmax = np.concatenate((test_softmax, test_softmax_tmp), axis=0)\n",
    "        # final feature stack by batch(subject)\n",
    "        test_feature_tmp = feature.cpu().data.numpy()\n",
    "        test_feature = np.concatenate((test_feature,test_feature_tmp), axis=0)\n",
    "           \n",
    "        # label, pred label, energy stack by batch(subject)\n",
    "        test_label_tmp = label_result.cpu().data.numpy()\n",
    "        test_pred_label_tmp = pred_label.cpu().data.numpy()\n",
    "        test_energy_tmp = energy\n",
    "        label_pred_energy_tmp = np.concatenate((test_label_tmp,\n",
    "                                               np.expand_dims(test_pred_label_tmp, 2),\n",
    "                                               np.expand_dims(test_energy_tmp, 2)), axis=2)\n",
    "        test_label_pred_energy = np.concatenate((test_label_pred_energy,label_pred_energy_tmp), axis=0)\n",
    "\n",
    "        # loss, acc\n",
    "        epoch_test_loss += loss\n",
    "        test_corrects += batch_corrects\n",
    "        test_L += batch_L\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Train results save\n",
    "    saved_by_epoch = os.path.join(saved_weights_folder,'epoch_{}'.format(epoch_num, '04'))\n",
    "    if not os.path.exists(saved_by_epoch):\n",
    "        os.makedirs(saved_by_epoch)\n",
    "        \n",
    "    # attention, softmax, final output feature save\n",
    "    test_att_np = test_att[1:,:]\n",
    "    test_softmax_np = test_softmax[1:,:,:]\n",
    "    test_feature_np = test_feature[1:,:,:]\n",
    "    np.save(saved_by_epoch+\"/test_att.npy\", test_att_np)\n",
    "    np.save(saved_by_epoch+\"/test_softmax.npy\", test_softmax_np)\n",
    "    np.save(saved_by_epoch+\"/test_feature.npy\", test_feature_np)\n",
    "    \n",
    "    # label, pred label, energy\n",
    "    test_label_pred_energy = test_label_pred_energy[1:,:,:]\n",
    "    np.save(saved_by_epoch+\"/test_pred_results.npy\", test_label_pred_energy)\n",
    "    \n",
    "    # loss, acc\n",
    "    epoch_test_loss = epoch_test_loss/num_step_per_epoch_test\n",
    "    avg_test_acc = test_corrects.float()/float(test_L)\n",
    "    print(\"epoch:{} \".format(epoch_num)+ \" test loss: {}\".format(epoch_test_loss))\n",
    "    print(\"epoch:{} \".format(epoch_num)+ \" test accuracy: {}\".format(avg_test_acc))\n",
    "    test_loss_list.append(epoch_test_loss)\n",
    "    test_acc_list.append(avg_test_acc)\n",
    "    \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(\"learning rate is : {}\".format(optimizer.param_groups[0]['lr']))\n",
    "    \n",
    "    torch.save(net.state_dict(), saved_by_epoch+'/model.pt')\n",
    "    torch.save(net.state_dict(), saved_by_epoch+'/model.pth')\n",
    "\n",
    "\n",
    "# train, test set subject name save\n",
    "train_subject_list, test_subject_list = k_fold(k=10,fold_num=fold_n,file_path=MESA_PPG_PATH)\n",
    "train_subjects = np.array(train_subject_list)\n",
    "test_subjects = np.array(test_subject_list)\n",
    "np.save(saved_weights_folder+\"/train_subjects.npy\", train_subjects)\n",
    "np.save(saved_weights_folder+\"/test_subjects.npy\", test_subjects)\n",
    "\n",
    "# final loss, acc save\n",
    "train_loss_np = np.array(train_loss_list)\n",
    "train_acc_np = np.array(train_acc_list)\n",
    "train_loss_np = np.expand_dims(train_loss_np, axis=1)\n",
    "train_acc_np = np.expand_dims(train_acc_np, axis=1)\n",
    "train_metric_results = np.concatenate((train_loss_np,train_acc_np), axis=1)\n",
    "train_metric_results_df = pd.DataFrame(train_metric_results)\n",
    "metric_header = ['loss', 'acc']\n",
    "train_metric_results_df.to_csv(saved_weights_folder+'/'+'train_metric_results.csv',header=metric_header, \n",
    "                               index=False)\n",
    "\n",
    "test_loss_np = np.array(test_loss_list)\n",
    "test_acc_np = np.array(test_acc_list)\n",
    "test_loss_np = np.expand_dims(test_loss_np, axis=1)\n",
    "test_acc_np = np.expand_dims(test_acc_np, axis=1)\n",
    "test_metric_results = np.concatenate((test_loss_np,test_acc_np), axis=1)\n",
    "test_metric_results_df = pd.DataFrame(test_metric_results)\n",
    "test_metric_results_df.to_csv(saved_weights_folder+'/'+'test_metric_results.csv',header=metric_header, \n",
    "                               index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
